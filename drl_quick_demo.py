#!/usr/bin/env python3
"""
Quick DRL Demo - Fast Results for Research
Train one config for 30 episodes to show proof of concept
"""

import numpy as np
import pandas as pd
import torch
import yfinance as yf
from deep_rl_trading import ProbDDPG
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

sns.set_style('whitegrid')

print('='*70)
print('ðŸ§  QUICK DRL DEMO - PROOF OF CONCEPT')
print('='*70)
print()

# Load EUR/USD data
print('ðŸ“Š Loading EUR/USD data...')
eur_data = yf.download('EURUSD=X', start='2020-01-01', progress=False)
eur_data['returns'] = eur_data['Close'].pct_change()
eur_data['vol_21d'] = eur_data['returns'].rolling(21).std()
eur_data['mom_21d'] = eur_data['Close'].pct_change(21)
eur_data['ma_5'] = eur_data['Close'].rolling(5).mean()
eur_data['ma_21'] = eur_data['Close'].rolling(21).mean()
eur_data['ma_cross'] = (eur_data['ma_5'] - eur_data['ma_21']) / eur_data['ma_21']
eur_data['range'] = (eur_data['High'] - eur_data['Low']) / eur_data['Close']
eur_data = eur_data.dropna()

print(f'âœ… Loaded {len(eur_data)} days')
print()

# Simple environment
class SimpleFXEnv:
    def __init__(self, data, capital=100000):
        self.data = data.reset_index(drop=True)
        self.capital = capital
        self.reset()
        
    def reset(self):
        self.position = 0
        self.equity = self.capital
        self.step_count = 21
        self.equity_curve = [self.capital]
        return self._get_state()
    
    def _get_state(self):
        idx = self.step_count
        if idx < 10:
            return np.zeros((10, 10))
        
        window = self.data.iloc[idx-10:idx]
        features = np.column_stack([
            window['returns'].values,
            window['vol_21d'].values,
            window['mom_21d'].values,
            window['ma_cross'].values,
            window['range'].values,
            window['Close'].pct_change(5).values,
            window['Close'].pct_change(10).values,
            np.full(10, self.position),
            np.full(10, self.equity / self.capital - 1),
            np.full(10, len(self.equity_curve) / 100)
        ])
        return features
    
    def step(self, action):
        current_price = self.data.iloc[self.step_count]['Close']
        target_position = np.clip(action, -1, 1)
        
        # Transaction cost
        cost = abs(target_position - self.position) * 0.0001 * self.capital
        old_position = self.position
        self.position = target_position
        
        self.step_count += 1
        if self.step_count >= len(self.data):
            return self._get_state(), 0, True
        
        next_price = self.data.iloc[self.step_count]['Close']
        price_change = (next_price - current_price) / current_price
        pnl = old_position * price_change * self.capital
        
        self.equity += pnl - cost
        self.equity_curve.append(self.equity)
        
        # Reward
        if len(self.equity_curve) > 21:
            returns = np.diff(self.equity_curve[-21:]) / self.capital
            reward = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)
        else:
            reward = pnl / self.capital
        
        done = self.step_count >= len(self.data) - 1
        return self._get_state(), reward, done

# Create environment
env = SimpleFXEnv(eur_data)

# Create agent
agent = ProbDDPG(
    input_dim=10,
    num_regimes=3,
    hidden_dim=64,
    lr_actor=0.0001,
    lr_critic=0.001
)

# Train for 30 episodes (quick)
print('ðŸš€ Training for 30 episodes (quick demo)...')
print('-'*70)

episode_sharpes = []
episode_returns = []
best_sharpe = -np.inf

for episode in range(30):
    observations = env.reset()
    done = False
    agent.gru_hidden = None  # Reset GRU hidden state
    
    while not done:
        # Filter regime from observations
        regime_probs = agent.filter_regime(observations)
        
        # Get market features (last row of observations)
        market_features = observations[-1, :5]  # First 5 features
        
        # Construct state
        state = agent.get_state(regime_probs, env.position, market_features)
        
        # Select action
        noise = 0.1 * (0.99 ** episode)
        action = agent.select_action(state, noise=noise)
        
        # Execute
        next_observations, reward, done = env.step(action)
        
        # Store transition (simplified - using last observation as state proxy)
        state_proxy = observations[-1]
        next_state_proxy = next_observations[-1]
        agent.memory.push(state_proxy, [action], reward, next_state_proxy, done)
        
        if len(agent.memory) > 64:
            agent.train_step(batch_size=64)
        
        observations = next_observations
    
    # Metrics
    equity_curve = np.array(env.equity_curve)
    returns = np.diff(equity_curve) / env.capital
    sharpe = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)
    total_return = (equity_curve[-1] / equity_curve[0] - 1) * 100
    
    episode_sharpes.append(sharpe)
    episode_returns.append(total_return)
    
    if sharpe > best_sharpe:
        best_sharpe = sharpe
        best_episode = episode
    
    print(f'Episode {episode+1:2d}: Sharpe={sharpe:+.3f}, Return={total_return:+.2f}%, Best={best_sharpe:+.3f}')

print()
print('='*70)
print('âœ… QUICK DEMO COMPLETE')
print('='*70)
print(f'Best Sharpe: {best_sharpe:+.3f} (Episode {best_episode+1})')
print(f'Final Sharpe: {episode_sharpes[-1]:+.3f}')
print(f'Final Return: {episode_returns[-1]:+.2f}%')
print()

# Plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

episodes = range(1, len(episode_sharpes) + 1)
ax1.plot(episodes, episode_sharpes, linewidth=2, color='steelblue')
ax1.axhline(y=0, color='black', linestyle='--', alpha=0.3)
ax1.axhline(y=best_sharpe, color='red', linestyle='--', alpha=0.3, label=f'Best: {best_sharpe:+.3f}')
ax1.scatter([best_episode+1], [best_sharpe], color='red', s=100, zorder=5)
ax1.set_xlabel('Episode', fontweight='bold')
ax1.set_ylabel('Sharpe Ratio', fontweight='bold')
ax1.set_title('DRL Learning Curve - Quick Demo', fontweight='bold')
ax1.legend()
ax1.grid(alpha=0.3)

ax2.plot(episodes, episode_returns, linewidth=2, color='forestgreen')
ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3)
ax2.set_xlabel('Episode', fontweight='bold')
ax2.set_ylabel('Return (%)', fontweight='bold')
ax2.set_title('Returns by Episode', fontweight='bold')
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('drl_quick_demo.png', dpi=150, bbox_inches='tight')
print('âœ… Saved: drl_quick_demo.png')
print()
print('ðŸ’¡ This shows DRL can learn to trade EUR/USD!')
print('   For full results, wait for comprehensive training to complete.')
