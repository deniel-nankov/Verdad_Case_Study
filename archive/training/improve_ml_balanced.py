#!/usr/bin/env python3
"""
BALANCED ML IMPROVEMENT - Best of Both Worlds
==============================================
Combines high returns of Kelly sizing with better risk management

Key improvements:
1. Feature selection (proven to work)
2. Conservative Kelly sizing (cap at 10% instead of 25%)
3. Volatility-adjusted positions (reduce size in high vol)
4. Better ensemble (keep what works from original)

Target: +3-5% return with Sharpe > 1.0
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, mutual_info_regression
import warnings
warnings.filterwarnings('ignore')

print('='*90)
print('âš–ï¸  BALANCED ML IMPROVEMENT - Optimize Return AND Sharpe')
print('='*90)
print()

# ============================================================================
# LOAD DATA
# ============================================================================

print('ğŸ“Š Loading EUR/USD data...')
eur_data = yf.download('EURUSD=X', start='2018-01-01', progress=False)

# Flatten multi-index columns if present
if isinstance(eur_data.columns, pd.MultiIndex):
    eur_data.columns = eur_data.columns.get_level_values(0)

eur_data['returns'] = eur_data['Close'].pct_change()
eur_data = eur_data.dropna()
print(f'âœ… Loaded {len(eur_data)} days of data')
print()

# ============================================================================
# SMART FEATURE ENGINEERING
# ============================================================================

def create_smart_features(df):
    """Create focused set of high-quality features"""
    features = pd.DataFrame(index=df.index)
    
    # Core momentum features
    for window in [5, 10, 21, 42, 63]:
        features[f'mom_{window}'] = df['Close'].pct_change(window)
    
    # Volatility features
    for window in [5, 10, 21, 42]:
        features[f'vol_{window}'] = df['returns'].rolling(window).std()
    
    # Moving averages (normalized)
    for window in [5, 10, 21, 42, 63, 126]:
        features[f'sma_{window}'] = (df['Close'] - df['Close'].rolling(window).mean()) / df['Close']
    
    # Volume features
    features['volume_ratio_5'] = df['Volume'] / df['Volume'].rolling(5).mean()
    features['volume_ratio_21'] = df['Volume'] / df['Volume'].rolling(21).mean()
    
    # Trend strength
    features['trend_21'] = (df['Close'].rolling(21).mean() - df['Close'].rolling(63).mean()) / df['Close']
    
    # RSI
    delta = df['Close'].diff()
    gain = delta.where(delta > 0, 0).rolling(14).mean()
    loss = -delta.where(delta < 0, 0).rolling(14).mean()
    rs = gain / (loss + 1e-8)
    features['rsi_14'] = 100 - (100 / (1 + rs))
    
    # Price position in range
    features['price_pos_21'] = (df['Close'] - df['Close'].rolling(21).min()) / (df['Close'].rolling(21).max() - df['Close'].rolling(21).min() + 1e-8)
    
    # Bollinger Band position
    sma_20 = df['Close'].rolling(20).mean()
    std_20 = df['Close'].rolling(20).std()
    features['bb_position'] = (df['Close'] - sma_20) / (2 * std_20 + 1e-8)
    
    # MACD
    ema_12 = df['Close'].ewm(span=12).mean()
    ema_26 = df['Close'].ewm(span=26).mean()
    features['macd'] = (ema_12 - ema_26) / df['Close']
    
    return features.fillna(0)

print('ğŸ”§ Creating smart features...')
features = create_smart_features(eur_data)
print(f'   Created {len(features.columns)} candidate features')

# ============================================================================
# FEATURE SELECTION
# ============================================================================

print()
print('ğŸ¯ Improvement 1: Feature Selection')

# Create target (21-day forward return)
target = eur_data['Close'].pct_change(21).shift(-21)

# Split data
train_mask = eur_data.index < '2024-01-01'
X_train = features[train_mask].iloc[:-21]
y_train = target[train_mask].iloc[:-21]

# Remove NaN
valid = ~y_train.isna()
X_train = X_train[valid]
y_train = y_train[valid]

# Select best 20 features (not 25 - keep it simpler)
selector = SelectKBest(mutual_info_regression, k=20)
selector.fit(X_train, y_train)

selected_features = X_train.columns[selector.get_support()].tolist()
print(f'   âœ… Selected top 20 features')
X_train = X_train[selected_features]

# ============================================================================
# TRAIN MODEL
# ============================================================================

print()
print('ğŸ¤– Training balanced ensemble...')

# Use original successful parameters from backtest_all_strategies.py
rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
xgb = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.05, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

rf.fit(X_train_scaled, y_train)
xgb.fit(X_train_scaled, y_train)

print(f'   âœ… Trained on {len(X_train)} samples')

# ============================================================================
# BALANCED POSITION SIZING
# ============================================================================

print()
print('âš–ï¸  Improvement 2: Balanced Position Sizing')
print('   Strategy: Conservative Kelly + Volatility Adjustment')

# Calculate training performance for Kelly
train_pred = 0.5 * rf.predict(X_train_scaled) + 0.5 * xgb.predict(X_train_scaled)
train_signals = np.sign(train_pred)
train_returns = train_signals * y_train

wins = train_returns[train_returns > 0]
losses = train_returns[train_returns < 0]

win_rate = len(wins) / (len(wins) + len(losses))
avg_win = wins.mean() if len(wins) > 0 else 0
avg_loss = abs(losses.mean()) if len(losses) > 0 else 0

# Kelly fraction (capped at 10% for safety)
if avg_loss > 0:
    kelly_frac = (win_rate * avg_win - (1 - win_rate) * avg_loss) / avg_win
    kelly_frac = np.clip(kelly_frac, 0, 0.10)  # MUCH more conservative
else:
    kelly_frac = 0.05

print(f'   Win rate: {win_rate*100:.1f}%')
print(f'   Kelly fraction: {kelly_frac:.3f} (conservative cap)')

# ============================================================================
# BACKTEST ON 2024-2025
# ============================================================================

print()
print('ğŸ“Š Backtesting balanced model...')
print()

# Prepare test data
X_test = features[~train_mask][selected_features]
y_test = target[~train_mask]

# Remove NaN
valid_test = ~y_test.isna()
X_test = X_test[valid_test]
y_test = y_test[valid_test]

# Calculate realized volatility for each period
test_dates = X_test.index
realized_vol = pd.Series(index=test_dates, dtype=float)
for date in test_dates:
    # Get last 21 days of returns before this date
    past_returns = eur_data.loc[:date, 'returns'].tail(21)
    realized_vol[date] = past_returns.std()

# Scale and predict
X_test_scaled = scaler.transform(X_test)
predictions = 0.5 * rf.predict(X_test_scaled) + 0.5 * xgb.predict(X_test_scaled)

# BALANCED POSITION SIZING:
# 1. Base position from prediction (like original model)
base_positions = np.clip(predictions * 10, -1, 1)

# 2. Apply conservative Kelly scaling
kelly_positions = base_positions * kelly_frac * 10  # Scale back up

# 3. Adjust for volatility (reduce in high vol periods)
median_vol = realized_vol.median()
vol_adjustment = median_vol / (realized_vol + 1e-8)
vol_adjustment = np.clip(vol_adjustment, 0.5, 1.5)  # Don't adjust too much

# Final positions
positions = kelly_positions * vol_adjustment
positions = np.clip(positions, -1, 1)

# Calculate returns
strategy_returns = positions * y_test

# Metrics
total_return = strategy_returns.sum()
sharpe = strategy_returns.mean() / (strategy_returns.std() + 1e-8) * np.sqrt(252)

# Drawdown
equity = (1 + strategy_returns).cumprod()
running_max = equity.expanding().max()
drawdown = (equity - running_max) / running_max
max_dd = drawdown.min()

# Win rate
winning_trades = strategy_returns[strategy_returns > 0]
win_rate = len(winning_trades) / len(strategy_returns)

# Count trades
position_changes = np.abs(np.diff(positions))
trades = np.sum(position_changes > 0.1)

print('='*90)
print('ğŸ“Š BALANCED ML MODEL RESULTS (2024-2025)')
print('='*90)
print()
print(f'   Total Return:    {total_return*100:+.2f}%')
print(f'   Sharpe Ratio:    {sharpe:+.3f}')
print(f'   Max Drawdown:    {max_dd*100:.2f}%')
print(f'   Win Rate:        {win_rate*100:.1f}%')
print(f'   Trades:          {trades}')
print()

# Comparison
print('ğŸ“ˆ COMPARISON:')
print(f'   Original ML:  +1.41% return, +0.896 Sharpe, -0.67% DD, 27 trades')
print(f'   Smart ML:     +12.00% return, +0.491 Sharpe, -52.51% DD, 197 trades')
print(f'   Balanced ML:  {total_return*100:+.2f}% return, {sharpe:+.3f} Sharpe, {max_dd*100:.2f}% DD, {trades} trades')
print()

# Evaluate
return_improvement = total_return * 100 - 1.41
sharpe_improvement = sharpe - 0.896

if return_improvement > 0 and sharpe_improvement > 0:
    print('   âœ… BEST OF BOTH WORLDS! Higher return AND better Sharpe')
elif total_return * 100 > 1.41 and sharpe > 0.5:
    print('   âœ… GOOD BALANCE - Much better returns with acceptable risk')
elif sharpe > 0.896:
    print('   âœ… BETTER RISK-ADJUSTED - Higher Sharpe but lower returns')
else:
    print('   âš ï¸  Still optimizing the balance...')

print()
print('='*90)
print('âœ… BALANCED MODEL COMPLETE!')
print('='*90)
print()
print('ğŸ’¡ Key improvements implemented:')
print('   âœ“ Feature selection (20 best features)')
print('   âœ“ Conservative Kelly sizing (10% cap vs 25%)')
print('   âœ“ Volatility adjustment (reduce risk in high vol)')
print('   âœ“ Proven ensemble weights (50/50 RF+XGB)')
